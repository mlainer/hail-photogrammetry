{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30424289",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "from detectron2.data.catalog import DatasetCatalog\n",
    "from detectron2.data.catalog import MetadataCatalog\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import random\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "import tensorflow as tf\n",
    "\n",
    "from detectron2.engine.hooks import HookBase\n",
    "from detectron2.evaluation import inference_context\n",
    "from detectron2.utils.logger import log_every_n_seconds\n",
    "from detectron2.data import DatasetMapper, build_detection_test_loader\n",
    "import detectron2.utils.comm as comm\n",
    "import torch\n",
    "import time\n",
    "import datetime\n",
    "import logging\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "\n",
    "from GPUtil import showUtilization as gpu_usage\n",
    "\n",
    "from detectron2.config import get_cfg\n",
    "cfg = get_cfg()\n",
    "\n",
    "#torch.cuda.set_per_process_memory_fraction(0.2)\n",
    "\n",
    "def train_hail_model(run_dir, base_lr, gamma, batch_size):\n",
    "    cfg.merge_from_file(\"/home/appuser/detectron2_repo/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
    "    \n",
    "    cfg.INPUT.MIN_SIZE_TRAIN = (500,)\n",
    "    \n",
    "    cfg.OUTPUT_DIR = run_dir\n",
    "    \n",
    "    cfg.DATASETS.TRAIN = (\"train_hail\",)\n",
    "    cfg.DATASETS.TEST = (\"val_hail\",)\n",
    "    \n",
    "    cfg.DATALOADER.NUM_WORKERS = 1\n",
    "\n",
    "    #cfg.INPUT.RANDOM_FLIP = \"horizontal\"\n",
    "    #cfg.SOLVER.CHECKPOINT_PERIOD = 2000\n",
    "\n",
    "    cfg.SOLVER.IMS_PER_BATCH = 2\n",
    "    cfg.SOLVER.BASE_LR = base_lr\n",
    "    cfg.SOLVER.WARMUP_ITERS = 50\n",
    "    cfg.SOLVER.MAX_ITER = 200 #adjust up if val AP is still rising, adjust down if overfit\n",
    "    cfg.SOLVER.STEPS = (80, 90)\n",
    "    cfg.SOLVER.GAMMA = gamma\n",
    "\n",
    "    # Test\n",
    "    cfg.TEST.EVAL_PERIOD = 100\n",
    "    cfg.TEST.DETECTIONS_PER_IMAGE = 50\n",
    "    \n",
    "    cfg.MODEL.DEVICE = \"cuda\"\n",
    "    cfg.MODEL.WEIGHTS = \"model_final_f10217.pkl\"  # initialize from model zoo\n",
    "    cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = batch_size\n",
    "    cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # 1 class (hail)\n",
    "    cfg.MODEL.MASK_ON = True\n",
    "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.05\n",
    "\n",
    "    os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "    trainer = CocoTrainer(cfg)\n",
    "    trainer.resume_or_load(resume=False)\n",
    "    trainer.train()\n",
    "    \n",
    "    output_dir = cfg.OUTPUT_DIR + '/eval/'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    evaluator = COCOEvaluator(\"val_hail\", cfg, False, output_dir=output_dir)\n",
    "    val_loader = build_detection_test_loader(cfg, \"val_hail\")\n",
    "    result = inference_on_dataset(trainer.model, val_loader, evaluator)\n",
    "\n",
    "\n",
    "def run(run_dir, hparams, base_lr, gamma, batch_size):\n",
    "    with tf.summary.create_file_writer(run_dir).as_default():\n",
    "        hp.hparams(hparams)  # record the values used in this trial\n",
    "        \n",
    "        cfg.merge_from_file(\"/home/appuser/detectron2_repo/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
    "    \n",
    "        cfg.INPUT.MIN_SIZE_TRAIN = (500,)\n",
    "\n",
    "        cfg.OUTPUT_DIR = run_dir\n",
    "\n",
    "        cfg.DATASETS.TRAIN = (\"train_hail\",)\n",
    "        cfg.DATASETS.TEST = (\"val_hail\",)\n",
    "\n",
    "        cfg.DATALOADER.NUM_WORKERS = 1\n",
    "\n",
    "        #cfg.INPUT.RANDOM_FLIP = \"horizontal\"\n",
    "        #cfg.SOLVER.CHECKPOINT_PERIOD = 2000\n",
    "\n",
    "        cfg.SOLVER.IMS_PER_BATCH = 2\n",
    "        cfg.SOLVER.BASE_LR = base_lr\n",
    "        cfg.SOLVER.WARMUP_ITERS = 50\n",
    "        cfg.SOLVER.MAX_ITER = 200 #adjust up if val AP is still rising, adjust down if overfit\n",
    "        cfg.SOLVER.STEPS = (80, 90)\n",
    "        cfg.SOLVER.GAMMA = gamma\n",
    "\n",
    "        # Test\n",
    "        cfg.TEST.EVAL_PERIOD = 100\n",
    "        cfg.TEST.DETECTIONS_PER_IMAGE = 50\n",
    "\n",
    "        cfg.MODEL.DEVICE = \"cuda\"\n",
    "        cfg.MODEL.WEIGHTS = \"model_final_f10217.pkl\"  # initialize from model zoo\n",
    "        cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = batch_size\n",
    "        cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # 1 class (hail)\n",
    "        cfg.MODEL.MASK_ON = True\n",
    "        cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.05\n",
    "\n",
    "        os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "        trainer = CocoTrainer(cfg)\n",
    "        trainer.resume_or_load(resume=False)\n",
    "        trainer.train()\n",
    "\n",
    "        output_dir = cfg.OUTPUT_DIR + '/eval/'\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        evaluator = COCOEvaluator(\"val_hail\", cfg, False, output_dir=output_dir)\n",
    "        val_loader = build_detection_test_loader(cfg, \"val_hail\")\n",
    "        result = inference_on_dataset(trainer.model, val_loader, evaluator)\n",
    "        \n",
    "        AP_bbox = result['bbox']['AP']\n",
    "        AP50_bbox = result['bbox']['AP50']\n",
    "        AP75_bbox = result['bbox']['AP75']\n",
    "        AP_segm = result['segm']['AP']\n",
    "        AP50_segm = result['segm']['AP50']\n",
    "        AP75_segm = result['segm']['AP75']\n",
    "        \n",
    "        del result\n",
    "        \n",
    "        tf.summary.scalar(AP_BBOX, AP_bbox, step=1)\n",
    "        tf.summary.scalar(AP50_BBOX, AP50_bbox, step=1)\n",
    "        tf.summary.scalar(AP75_BBOX, AP75_bbox, step=1)\n",
    "        tf.summary.scalar(AP_SEGM, AP_segm, step=1)\n",
    "        tf.summary.scalar(AP50_SEGM, AP50_segm, step=1)\n",
    "        tf.summary.scalar(AP75_SEGM, AP75_segm, step=1)\n",
    "        \n",
    "        del AP_bbox, AP50_bbox, AP75_bbox, AP_segm, AP50_segm, AP75_segm\n",
    "        \n",
    "        \n",
    "class CocoTrainer(DefaultTrainer):\n",
    "\n",
    "  @classmethod\n",
    "  def build_evaluator(cls, cfg, dataset_name, output_folder=cfg.OUTPUT_DIR):\n",
    "    \n",
    "    return COCOEvaluator(dataset_name, cfg, False, output_folder)\n",
    "\n",
    "  def build_hooks(self):\n",
    "        \n",
    "    hooks = super().build_hooks()\n",
    "    hooks.insert(-1,LossEvalHook(\n",
    "        cfg.TEST.EVAL_PERIOD,\n",
    "        self.model,\n",
    "        build_detection_test_loader(\n",
    "            self.cfg,\n",
    "            self.cfg.DATASETS.TEST[0],\n",
    "            DatasetMapper(self.cfg,True)\n",
    "        )\n",
    "    ))\n",
    "    return hooks\n",
    "\n",
    "class LossEvalHook(HookBase):\n",
    "    def __init__(self, eval_period, model, data_loader):\n",
    "        self._model = model\n",
    "        self._period = eval_period\n",
    "        self._data_loader = data_loader\n",
    "    \n",
    "    def _do_loss_eval(self):\n",
    "        # Copying inference_on_dataset from evaluator.py\n",
    "        total = len(self._data_loader)\n",
    "        num_warmup = min(5, total - 1)\n",
    "            \n",
    "        start_time = time.perf_counter()\n",
    "        total_compute_time = 0\n",
    "        losses = []\n",
    "        for idx, inputs in enumerate(self._data_loader):            \n",
    "            if idx == num_warmup:\n",
    "                start_time = time.perf_counter()\n",
    "                total_compute_time = 0\n",
    "            start_compute_time = time.perf_counter()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "            total_compute_time += time.perf_counter() - start_compute_time\n",
    "            iters_after_start = idx + 1 - num_warmup * int(idx >= num_warmup)\n",
    "            seconds_per_img = total_compute_time / iters_after_start\n",
    "            if idx >= num_warmup * 2 or seconds_per_img > 5:\n",
    "                total_seconds_per_img = (time.perf_counter() - start_time) / iters_after_start\n",
    "                eta = datetime.timedelta(seconds=int(total_seconds_per_img * (total - idx - 1)))\n",
    "                log_every_n_seconds(\n",
    "                    logging.INFO,\n",
    "                    \"Loss on Validation  done {}/{}. {:.4f} s / img. ETA={}\".format(\n",
    "                        idx + 1, total, seconds_per_img, str(eta)\n",
    "                    ),\n",
    "                    n=5,\n",
    "                )\n",
    "            loss_batch = self._get_loss(inputs)\n",
    "            losses.append(loss_batch)\n",
    "        mean_loss = np.mean(losses)\n",
    "        self.trainer.storage.put_scalar('validation_loss', mean_loss)\n",
    "        comm.synchronize()\n",
    "\n",
    "        return losses\n",
    "            \n",
    "    def _get_loss(self, data):\n",
    "        # How loss is calculated on train_loop \n",
    "        metrics_dict = self._model(data)\n",
    "        metrics_dict = {\n",
    "            k: v.detach().cpu().item() if isinstance(v, torch.Tensor) else float(v)\n",
    "            for k, v in metrics_dict.items()\n",
    "        }\n",
    "        total_losses_reduced = sum(loss for loss in metrics_dict.values())\n",
    "        return total_losses_reduced\n",
    "             \n",
    "    def after_step(self):\n",
    "        next_iter = self.trainer.iter + 1\n",
    "        is_final = next_iter == self.trainer.max_iter\n",
    "        if is_final or (self._period > 0 and next_iter % self._period == 0):\n",
    "            self._do_loss_eval()\n",
    "        self.trainer.storage.put_scalars(timetest=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d5931f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90ae0c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting trial: run-0\n",
      "{'base_lr': 0.0001, 'gamma': 0.1, 'batch_size_per_image': 128}\n",
      "\u001b[32m[07/22 11:13:45 d2.engine.defaults]: \u001b[0mModel:\n",
      "GeneralizedRCNN(\n",
      "  (backbone): FPN(\n",
      "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (top_block): LastLevelMaxPool()\n",
      "    (bottom_up): ResNet(\n",
      "      (stem): BasicStem(\n",
      "        (conv1): Conv2d(\n",
      "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (res2): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res3): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res4): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (4): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (5): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res5): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (proposal_generator): RPN(\n",
      "    (rpn_head): StandardRPNHead(\n",
      "      (conv): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (anchor_generator): DefaultAnchorGenerator(\n",
      "      (cell_anchors): BufferList()\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): StandardROIHeads(\n",
      "    (box_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (box_head): FastRCNNConvFCHead(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc_relu1): ReLU()\n",
      "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (fc_relu2): ReLU()\n",
      "    )\n",
      "    (box_predictor): FastRCNNOutputLayers(\n",
      "      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)\n",
      "    )\n",
      "    (mask_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (mask_head): MaskRCNNConvUpsampleHead(\n",
      "      (mask_fcn1): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (mask_fcn2): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (mask_fcn3): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (mask_fcn4): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (deconv_relu): ReLU()\n",
      "      (predictor): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[07/22 11:13:45 d2.data.datasets.coco]: \u001b[0mLoaded 150 images in COCO format from ./data/hail_20210620_r1/train/annotations/instances_default.json\n",
      "\u001b[32m[07/22 11:13:45 d2.data.build]: \u001b[0mRemoved 2 images with no usable annotations. 148 images left.\n",
      "\u001b[32m[07/22 11:13:45 d2.data.build]: \u001b[0mDistribution of instances among all 1 categories:\n",
      "\u001b[36m|  category  | #instances   |\n",
      "|:----------:|:-------------|\n",
      "|    hail    | 937          |\n",
      "|            |              |\u001b[0m\n",
      "\u001b[32m[07/22 11:13:45 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(500,), max_size=1333, sample_style='choice'), RandomFlip()]\n",
      "\u001b[32m[07/22 11:13:45 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n",
      "\u001b[32m[07/22 11:13:45 d2.data.common]: \u001b[0mSerializing 148 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/22 11:13:45 d2.data.common]: \u001b[0mSerialized dataset takes 0.34 MiB\n",
      "\u001b[32m[07/22 11:13:45 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(500,), max_size=1333, sample_style='choice'), RandomFlip()]\n",
      "\u001b[32m[07/22 11:13:45 d2.data.datasets.coco]: \u001b[0mLoaded 33 images in COCO format from ./data/hail_20210620_r1/val/annotations/instances_default.json\n",
      "\u001b[32m[07/22 11:13:45 d2.data.build]: \u001b[0mDistribution of instances among all 1 categories:\n",
      "\u001b[36m|  category  | #instances   |\n",
      "|:----------:|:-------------|\n",
      "|    hail    | 249          |\n",
      "|            |              |\u001b[0m\n",
      "\u001b[32m[07/22 11:13:45 d2.data.common]: \u001b[0mSerializing 33 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/22 11:13:45 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (2, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (2,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (4, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (4,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.mask_head.predictor.weight' to the model due to incompatible shapes: (80, 256, 1, 1) in the checkpoint but (1, 256, 1, 1) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.mask_head.predictor.bias' to the model due to incompatible shapes: (80,) in the checkpoint but (1,) in the model! You might want to double check if this is expected.\n",
      "Some model parameters or buffers are not found in the checkpoint:\n",
      "\u001b[34mroi_heads.box_predictor.bbox_pred.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_predictor.cls_score.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.mask_head.predictor.{bias, weight}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[07/22 11:13:45 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/appuser/.local/lib/python3.6/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[07/22 11:13:48 d2.utils.events]: \u001b[0m eta: 0:00:19  iter: 19  total_loss: 5.619  loss_cls: 0.8687  loss_box_reg: 0.3974  loss_mask: 0.6954  loss_rpn_cls: 3.178  loss_rpn_loc: 0.5747  time: 0.1153  data_time: 0.0076  lr: 3.8062e-05  max_mem: 1083M\n",
      "\u001b[32m[07/22 11:13:50 d2.utils.events]: \u001b[0m eta: 0:00:18  iter: 39  total_loss: 2.388  loss_cls: 0.4649  loss_box_reg: 0.7612  loss_mask: 0.6167  loss_rpn_cls: 0.25  loss_rpn_loc: 0.2425  time: 0.1150  data_time: 0.0028  lr: 7.8022e-05  max_mem: 1083M\n",
      "\u001b[32m[07/22 11:13:52 d2.utils.events]: \u001b[0m eta: 0:00:16  iter: 59  total_loss: 2.007  loss_cls: 0.3656  loss_box_reg: 0.7932  loss_mask: 0.5135  loss_rpn_cls: 0.09244  loss_rpn_loc: 0.2186  time: 0.1157  data_time: 0.0027  lr: 0.0001  max_mem: 1083M\n",
      "\u001b[32m[07/22 11:13:55 d2.utils.events]: \u001b[0m eta: 0:00:13  iter: 79  total_loss: 1.874  loss_cls: 0.3041  loss_box_reg: 0.7779  loss_mask: 0.4753  loss_rpn_cls: 0.09315  loss_rpn_loc: 0.2078  time: 0.1162  data_time: 0.0034  lr: 0.0001  max_mem: 1083M\n",
      "\u001b[32m[07/22 11:13:57 d2.data.datasets.coco]: \u001b[0mLoaded 33 images in COCO format from ./data/hail_20210620_r1/val/annotations/instances_default.json\n",
      "\u001b[32m[07/22 11:13:57 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[07/22 11:13:57 d2.data.common]: \u001b[0mSerializing 33 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/22 11:13:57 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
      "\u001b[32m[07/22 11:13:57 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/22 11:13:57 d2.evaluation.coco_evaluation]: \u001b[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001b[32m[07/22 11:13:57 d2.evaluation.evaluator]: \u001b[0mStart inference on 33 batches\n",
      "\u001b[32m[07/22 11:13:58 d2.evaluation.evaluator]: \u001b[0mInference done 11/33. Dataloading: 0.0007 s/iter. Inference: 0.0581 s/iter. Eval: 0.0146 s/iter. Total: 0.0734 s/iter. ETA=0:00:01\n",
      "\u001b[32m[07/22 11:13:59 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:02.043052 (0.072966 s / iter per device, on 1 devices)\n",
      "\u001b[32m[07/22 11:13:59 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:01 (0.055437 s / iter per device, on 1 devices)\n",
      "\u001b[32m[07/22 11:13:59 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[07/22 11:13:59 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/coco_instances_results.json\n",
      "\u001b[32m[07/22 11:13:59 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.32s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.303\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.786\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.089\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.303\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.067\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.372\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.436\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.436\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "\u001b[32m[07/22 11:14:00 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm  |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:-----:|:-----:|\n",
      "| 30.294 | 78.577 | 8.891  | 30.294 |  nan  |  nan  |\n",
      "\u001b[32m[07/22 11:14:00 d2.evaluation.coco_evaluation]: \u001b[0mSome metrics cannot be computed and is shown as NaN.\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=0.34s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.275\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.753\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.096\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.275\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.061\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.348\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.415\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.415\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "\u001b[32m[07/22 11:14:00 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for segm: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm  |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:-----:|:-----:|\n",
      "| 27.549 | 75.287 | 9.612  | 27.549 |  nan  |  nan  |\n",
      "\u001b[32m[07/22 11:14:00 d2.evaluation.coco_evaluation]: \u001b[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001b[32m[07/22 11:14:00 d2.engine.defaults]: \u001b[0mEvaluation results for val_hail in csv format:\n",
      "\u001b[32m[07/22 11:14:00 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
      "\u001b[32m[07/22 11:14:00 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
      "\u001b[32m[07/22 11:14:00 d2.evaluation.testing]: \u001b[0mcopypaste: 30.2938,78.5765,8.8911,30.2938,nan,nan\n",
      "\u001b[32m[07/22 11:14:00 d2.evaluation.testing]: \u001b[0mcopypaste: Task: segm\n",
      "\u001b[32m[07/22 11:14:00 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
      "\u001b[32m[07/22 11:14:00 d2.evaluation.testing]: \u001b[0mcopypaste: 27.5486,75.2871,9.6124,27.5486,nan,nan\n",
      "\u001b[32m[07/22 11:14:01 detectron2]: \u001b[0mLoss on Validation  done 11/33. 0.0000 s / img. ETA=0:00:00\n",
      "\u001b[32m[07/22 11:14:02 d2.utils.events]: \u001b[0m eta: 0:00:11  iter: 99  total_loss: 1.734  loss_cls: 0.2657  loss_box_reg: 0.7306  loss_mask: 0.4473  loss_rpn_cls: 0.08651  loss_rpn_loc: 0.2081  validation_loss: 1.853  time: 0.1164  data_time: 0.0025  lr: 1e-06  max_mem: 1083M\n",
      "\u001b[32m[07/22 11:14:04 d2.utils.events]: \u001b[0m eta: 0:00:09  iter: 119  total_loss: 1.704  loss_cls: 0.2646  loss_box_reg: 0.7043  loss_mask: 0.4516  loss_rpn_cls: 0.08349  loss_rpn_loc: 0.1915  validation_loss: 1.853  time: 0.1161  data_time: 0.0029  lr: 1e-06  max_mem: 1083M\n",
      "\u001b[32m[07/22 11:14:06 d2.utils.events]: \u001b[0m eta: 0:00:06  iter: 139  total_loss: 1.776  loss_cls: 0.2873  loss_box_reg: 0.75  loss_mask: 0.462  loss_rpn_cls: 0.1005  loss_rpn_loc: 0.2094  validation_loss: 1.853  time: 0.1163  data_time: 0.0028  lr: 1e-06  max_mem: 1083M\n",
      "\u001b[32m[07/22 11:14:09 d2.utils.events]: \u001b[0m eta: 0:00:04  iter: 159  total_loss: 1.712  loss_cls: 0.2509  loss_box_reg: 0.7388  loss_mask: 0.4491  loss_rpn_cls: 0.07965  loss_rpn_loc: 0.178  validation_loss: 1.853  time: 0.1167  data_time: 0.0028  lr: 1e-06  max_mem: 1083M\n",
      "\u001b[32m[07/22 11:14:11 d2.utils.events]: \u001b[0m eta: 0:00:02  iter: 179  total_loss: 1.815  loss_cls: 0.295  loss_box_reg: 0.7438  loss_mask: 0.47  loss_rpn_cls: 0.1  loss_rpn_loc: 0.2142  validation_loss: 1.853  time: 0.1171  data_time: 0.0027  lr: 1e-06  max_mem: 1083M\n",
      "\u001b[32m[07/22 11:14:15 detectron2]: \u001b[0mLoss on Validation  done 11/33. 0.0000 s / img. ETA=0:00:00\n",
      "\u001b[32m[07/22 11:14:16 d2.utils.events]: \u001b[0m eta: 0:00:00  iter: 199  total_loss: 1.612  loss_cls: 0.265  loss_box_reg: 0.7071  loss_mask: 0.4422  loss_rpn_cls: 0.08266  loss_rpn_loc: 0.179  validation_loss: 1.846  time: 0.1173  data_time: 0.0028  lr: 1e-06  max_mem: 1083M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[07/22 11:14:16 d2.engine.hooks]: \u001b[0mOverall training speed: 198 iterations in 0:00:23 (0.1173 s / it)\n",
      "\u001b[32m[07/22 11:14:16 d2.engine.hooks]: \u001b[0mTotal training time: 0:00:30 (0:00:06 on hooks)\n",
      "\u001b[32m[07/22 11:14:16 d2.data.datasets.coco]: \u001b[0mLoaded 33 images in COCO format from ./data/hail_20210620_r1/val/annotations/instances_default.json\n",
      "\u001b[32m[07/22 11:14:16 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[07/22 11:14:16 d2.data.common]: \u001b[0mSerializing 33 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/22 11:14:16 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
      "\u001b[32m[07/22 11:14:16 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/22 11:14:16 d2.evaluation.coco_evaluation]: \u001b[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001b[32m[07/22 11:14:16 d2.evaluation.evaluator]: \u001b[0mStart inference on 33 batches\n",
      "\u001b[32m[07/22 11:14:16 d2.evaluation.evaluator]: \u001b[0mInference done 11/33. Dataloading: 0.0007 s/iter. Inference: 0.0591 s/iter. Eval: 0.0144 s/iter. Total: 0.0741 s/iter. ETA=0:00:01\n",
      "\u001b[32m[07/22 11:14:18 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:02.050341 (0.073226 s / iter per device, on 1 devices)\n",
      "\u001b[32m[07/22 11:14:18 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:01 (0.055817 s / iter per device, on 1 devices)\n",
      "\u001b[32m[07/22 11:14:18 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[07/22 11:14:18 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/coco_instances_results.json\n",
      "\u001b[32m[07/22 11:14:18 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.32s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.298\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.780\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.088\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.298\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.065\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.370\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.434\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.434\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "\u001b[32m[07/22 11:14:18 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm  |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:-----:|:-----:|\n",
      "| 29.752 | 77.982 | 8.790  | 29.752 |  nan  |  nan  |\n",
      "\u001b[32m[07/22 11:14:18 d2.evaluation.coco_evaluation]: \u001b[0mSome metrics cannot be computed and is shown as NaN.\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=0.34s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.289\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.756\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.127\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.289\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.062\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.360\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.427\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.427\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "\u001b[32m[07/22 11:14:19 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for segm: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm  |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:-----:|:-----:|\n",
      "| 28.937 | 75.555 | 12.721 | 28.937 |  nan  |  nan  |\n",
      "\u001b[32m[07/22 11:14:19 d2.evaluation.coco_evaluation]: \u001b[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001b[32m[07/22 11:14:19 d2.engine.defaults]: \u001b[0mEvaluation results for val_hail in csv format:\n",
      "\u001b[32m[07/22 11:14:19 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
      "\u001b[32m[07/22 11:14:19 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
      "\u001b[32m[07/22 11:14:19 d2.evaluation.testing]: \u001b[0mcopypaste: 29.7521,77.9824,8.7901,29.7522,nan,nan\n",
      "\u001b[32m[07/22 11:14:19 d2.evaluation.testing]: \u001b[0mcopypaste: Task: segm\n",
      "\u001b[32m[07/22 11:14:19 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
      "\u001b[32m[07/22 11:14:19 d2.evaluation.testing]: \u001b[0mcopypaste: 28.9369,75.5552,12.7210,28.9369,nan,nan\n",
      "\u001b[32m[07/22 11:14:19 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/22 11:14:19 d2.evaluation.coco_evaluation]: \u001b[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001b[32m[07/22 11:14:19 d2.data.datasets.coco]: \u001b[0mLoaded 33 images in COCO format from ./data/hail_20210620_r1/val/annotations/instances_default.json\n",
      "\u001b[32m[07/22 11:14:19 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[07/22 11:14:19 d2.data.common]: \u001b[0mSerializing 33 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/22 11:14:19 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
      "\u001b[32m[07/22 11:14:19 d2.evaluation.evaluator]: \u001b[0mStart inference on 33 batches\n",
      "\u001b[32m[07/22 11:14:21 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:02.067828 (0.073851 s / iter per device, on 1 devices)\n",
      "\u001b[32m[07/22 11:14:21 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:01 (0.056321 s / iter per device, on 1 devices)\n",
      "\u001b[32m[07/22 11:14:21 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[07/22 11:14:21 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to output/logs/hparam_tuning/run-0/eval/coco_instances_results.json\n",
      "\u001b[32m[07/22 11:14:21 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.31s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.298\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.780\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.088\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.298\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.065\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.370\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.434\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.434\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "\u001b[32m[07/22 11:14:22 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm  |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:-----:|:-----:|\n",
      "| 29.752 | 77.982 | 8.790  | 29.752 |  nan  |  nan  |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[07/22 11:14:22 d2.evaluation.coco_evaluation]: \u001b[0mSome metrics cannot be computed and is shown as NaN.\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=0.34s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.289\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.756\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.127\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.289\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.062\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.360\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.427\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.427\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "\u001b[32m[07/22 11:14:22 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for segm: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm  |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:-----:|:-----:|\n",
      "| 28.937 | 75.555 | 12.721 | 28.937 |  nan  |  nan  |\n",
      "\u001b[32m[07/22 11:14:22 d2.evaluation.coco_evaluation]: \u001b[0mSome metrics cannot be computed and is shown as NaN.\n",
      "--- Starting trial: run-1\n",
      "{'base_lr': 0.0001, 'gamma': 0.1, 'batch_size_per_image': 256}\n",
      "\u001b[32m[07/22 11:14:23 d2.engine.defaults]: \u001b[0mModel:\n",
      "GeneralizedRCNN(\n",
      "  (backbone): FPN(\n",
      "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (top_block): LastLevelMaxPool()\n",
      "    (bottom_up): ResNet(\n",
      "      (stem): BasicStem(\n",
      "        (conv1): Conv2d(\n",
      "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (res2): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res3): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res4): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (4): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (5): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res5): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (proposal_generator): RPN(\n",
      "    (rpn_head): StandardRPNHead(\n",
      "      (conv): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (anchor_generator): DefaultAnchorGenerator(\n",
      "      (cell_anchors): BufferList()\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): StandardROIHeads(\n",
      "    (box_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (box_head): FastRCNNConvFCHead(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc_relu1): ReLU()\n",
      "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (fc_relu2): ReLU()\n",
      "    )\n",
      "    (box_predictor): FastRCNNOutputLayers(\n",
      "      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)\n",
      "    )\n",
      "    (mask_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (mask_head): MaskRCNNConvUpsampleHead(\n",
      "      (mask_fcn1): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (mask_fcn2): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (mask_fcn3): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (mask_fcn4): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (deconv_relu): ReLU()\n",
      "      (predictor): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[07/22 11:14:23 d2.data.datasets.coco]: \u001b[0mLoaded 150 images in COCO format from ./data/hail_20210620_r1/train/annotations/instances_default.json\n",
      "\u001b[32m[07/22 11:14:23 d2.data.build]: \u001b[0mRemoved 2 images with no usable annotations. 148 images left.\n",
      "\u001b[32m[07/22 11:14:23 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(500,), max_size=1333, sample_style='choice'), RandomFlip()]\n",
      "\u001b[32m[07/22 11:14:23 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n",
      "\u001b[32m[07/22 11:14:23 d2.data.common]: \u001b[0mSerializing 148 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/22 11:14:23 d2.data.common]: \u001b[0mSerialized dataset takes 0.34 MiB\n",
      "\u001b[32m[07/22 11:14:23 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(500,), max_size=1333, sample_style='choice'), RandomFlip()]\n",
      "\u001b[32m[07/22 11:14:23 d2.data.datasets.coco]: \u001b[0mLoaded 33 images in COCO format from ./data/hail_20210620_r1/val/annotations/instances_default.json\n",
      "\u001b[32m[07/22 11:14:23 d2.data.common]: \u001b[0mSerializing 33 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/22 11:14:23 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (2, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (2,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (4, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (4,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.mask_head.predictor.weight' to the model due to incompatible shapes: (80, 256, 1, 1) in the checkpoint but (1, 256, 1, 1) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.mask_head.predictor.bias' to the model due to incompatible shapes: (80,) in the checkpoint but (1,) in the model! You might want to double check if this is expected.\n",
      "Some model parameters or buffers are not found in the checkpoint:\n",
      "\u001b[34mroi_heads.box_predictor.bbox_pred.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_predictor.cls_score.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.mask_head.predictor.{bias, weight}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[07/22 11:14:23 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n",
      "\u001b[32m[07/22 11:14:25 d2.utils.events]: \u001b[0m eta: 0:00:20  iter: 19  total_loss: 5.356  loss_cls: 0.736  loss_box_reg: 0.2042  loss_mask: 0.688  loss_rpn_cls: 3.283  loss_rpn_loc: 0.5339  time: 0.1176  data_time: 0.0076  lr: 3.8062e-05  max_mem: 1199M\n",
      "\u001b[4m\u001b[5m\u001b[31mERROR\u001b[0m \u001b[32m[07/22 11:14:26 d2.engine.train_loop]: \u001b[0mException during training:\n",
      "Traceback (most recent call last):\n",
      "  File \"/container/hail/detectron2/detectron2/engine/train_loop.py\", line 149, in train\n",
      "    self.run_step()\n",
      "  File \"/container/hail/detectron2/detectron2/engine/defaults.py\", line 494, in run_step\n",
      "    self._trainer.run_step()\n",
      "  File \"/container/hail/detectron2/detectron2/engine/train_loop.py\", line 285, in run_step\n",
      "    losses.backward()\n",
      "  File \"/home/appuser/.local/lib/python3.6/site-packages/torch/_tensor.py\", line 307, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n",
      "  File \"/home/appuser/.local/lib/python3.6/site-packages/torch/autograd/__init__.py\", line 156, in backward\n",
      "    allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 7.80 GiB total capacity; 1.17 GiB already allocated; 36.62 MiB free; 1.43 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\u001b[32m[07/22 11:14:26 d2.engine.hooks]: \u001b[0mOverall training speed: 22 iterations in 0:00:02 (0.1240 s / it)\n",
      "\u001b[32m[07/22 11:14:26 d2.engine.hooks]: \u001b[0mTotal training time: 0:00:02 (0:00:00 on hooks)\n",
      "\u001b[32m[07/22 11:14:26 d2.utils.events]: \u001b[0m eta: 0:00:20  iter: 24  total_loss: 3.737  loss_cls: 0.6622  loss_box_reg: 0.2446  loss_mask: 0.6762  loss_rpn_cls: 1.592  loss_rpn_loc: 0.4791  time: 0.1188  data_time: 0.0030  lr: 4.6054e-05  max_mem: 1230M\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 7.80 GiB total capacity; 1.17 GiB already allocated; 36.62 MiB free; 1.43 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a4ad42e37dc6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'--- Starting trial: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mrun_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhparams\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'output/logs/hparam_tuning/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrun_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size_per_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m             \u001b[0msession_num\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-1d9328ae8257>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(run_dir, hparams, base_lr, gamma, batch_size)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCocoTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresume_or_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0moutput_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOUTPUT_DIR\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/eval/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/container/hail/detectron2/detectron2/engine/defaults.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    482\u001b[0m             \u001b[0mOrderedDict\u001b[0m \u001b[0mof\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mevaluation\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0menabled\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mOtherwise\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m         \"\"\"\n\u001b[0;32m--> 484\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTEST\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXPECTED_RESULTS\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcomm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_main_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             assert hasattr(\n",
      "\u001b[0;32m/container/hail/detectron2/detectron2/engine/train_loop.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, start_iter, max_iter)\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m                 \u001b[0;31m# self.iter == max_iter can be used by `after_train` to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/container/hail/detectron2/detectron2/engine/defaults.py\u001b[0m in \u001b[0;36mrun_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    492\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/container/hail/detectron2/detectron2/engine/train_loop.py\u001b[0m in \u001b[0;36mrun_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    283\u001b[0m         \"\"\"\n\u001b[1;32m    284\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_write_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 7.80 GiB total capacity; 1.17 GiB already allocated; 36.62 MiB free; 1.43 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "register = 1\n",
    "if register == 1:\n",
    "    #Register train data set\n",
    "    register_coco_instances(\"train_hail\", {}, \"./data/hail_20210620_r1/train/annotations/instances_default.json\", \"./data/hail_20210620_r1/train/images\")\n",
    "    dataset_dicts_train = DatasetCatalog.get(\"train_hail\")\n",
    "    hail_metadata_train = MetadataCatalog.get(\"train_hail\")\n",
    "\n",
    "    #Register validation data set\n",
    "    register_coco_instances(\"val_hail\", {}, \"./data/hail_20210620_r1/val/annotations/instances_default.json\", \"./data/hail_20210620_r1/val/images\")\n",
    "    dataset_dicts_val = DatasetCatalog.get(\"val_hail\")\n",
    "    hail_metadata_val = MetadataCatalog.get(\"val_hail\")\n",
    "\n",
    "    #Register test data set\n",
    "    register_coco_instances(\"test_hail\", {}, \"./data/hail_20210620_r1/test/annotations/instances_default.json\", \"./data/hail_20210620_r1/test/images\")\n",
    "    dataset_dicts_test = DatasetCatalog.get(\"test_hail\")\n",
    "    hail_metadata_test = MetadataCatalog.get(\"test_hail\")\n",
    "\n",
    "HP_BASE_LR = hp.HParam('base_lr', hp.Discrete([0.0001, 0.00025, 0.0005, 0.001]))\n",
    "HP_GAMMA = hp.HParam('gamma', hp.Discrete([0.1,0.5]))\n",
    "HP_BATCH_SIZE_PER_IMAGE = hp.HParam('batch_size_per_image', hp.Discrete([128, 256]))\n",
    "\n",
    "AP_BBOX = 'AP_bbox'\n",
    "AP50_BBOX = 'AP50_bbox'\n",
    "AP75_BBOX = 'AP75_bbox'\n",
    "\n",
    "AP_SEGM = 'AP_segm'\n",
    "AP50_SEGM = 'AP50_segm'\n",
    "AP75_SEGM = 'AP75_segm'\n",
    "\n",
    "with tf.summary.create_file_writer('output/logs/hparam_tuning').as_default():\n",
    "    hp.hparams_config(\n",
    "        hparams=[HP_BASE_LR, HP_GAMMA, HP_BATCH_SIZE_PER_IMAGE],\n",
    "        metrics=[hp.Metric(AP_BBOX, display_name='AP_bbox'), \n",
    "                 hp.Metric(AP50_BBOX, display_name='AP50_bbox'), \n",
    "                 hp.Metric(AP75_BBOX, display_name='AP75_bbox'),\n",
    "                 hp.Metric(AP_SEGM, display_name='AP_segm'), \n",
    "                 hp.Metric(AP50_SEGM, display_name='AP50_segm'), \n",
    "                 hp.Metric(AP75_SEGM, display_name='AP75_segm')\n",
    "                 ],\n",
    "    ) \n",
    "\n",
    "session_num = 0\n",
    "for base_lr in HP_BASE_LR.domain.values:\n",
    "    for gamma in HP_GAMMA.domain.values:\n",
    "        for batch_size_per_image in HP_BATCH_SIZE_PER_IMAGE.domain.values:\n",
    "            hparams = {\n",
    "                HP_BASE_LR: base_lr,\n",
    "                HP_GAMMA: gamma,\n",
    "                HP_BATCH_SIZE_PER_IMAGE: batch_size_per_image,\n",
    "                }\n",
    "            run_name = \"run-%d\" % session_num\n",
    "            print('--- Starting trial: %s' % run_name)\n",
    "            print({h.name: hparams[h] for h in hparams})\n",
    "            run('output/logs/hparam_tuning/' + run_name, hparams, base_lr, gamma, batch_size_per_image)\n",
    "            session_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7365c7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting numba\n",
      "  Downloading numba-0.53.1-cp36-cp36m-manylinux2014_x86_64.whl (3.4 MB)\n",
      "     |################################| 3.4 MB 1.8 MB/s            \n",
      "\u001b[?25hCollecting llvmlite<0.37,>=0.36.0rc1\n",
      "  Downloading llvmlite-0.36.0-cp36-cp36m-manylinux2010_x86_64.whl (25.3 MB)\n",
      "     |################################| 25.3 MB 6.8 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.15 in /home/appuser/.local/lib/python3.6/site-packages (from numba) (1.19.5)\n",
      "Requirement already satisfied: setuptools in /home/appuser/.local/lib/python3.6/site-packages (from numba) (59.6.0)\n",
      "Installing collected packages: llvmlite, numba\n",
      "Successfully installed llvmlite-0.36.0 numba-0.53.1\n"
     ]
    }
   ],
   "source": [
    "!pip install numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "925efbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "788bc1b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 21% |\n",
      "GPU Usage after emptying the cache\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 17% |  2% |\n"
     ]
    }
   ],
   "source": [
    "free_gpu_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "84f9ece4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Detect hail and save segmentation masks\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog, build_detection_test_loader\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.engine import DefaultPredictor\n",
    "import numpy as np\n",
    "\n",
    "from detectron2.utils.visualizer import ColorMode\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "experiment_folder = './output/hail_20210620_r1_test_4/'\n",
    "\n",
    "cfg.MODEL.WEIGHTS = os.path.join(experiment_folder, \"model_0014999.pth\")\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.80\n",
    "predictor = DefaultPredictor(cfg)\n",
    "\n",
    "images_path = 'data/hail_20210620_r1/all_images/'\n",
    "mask_array_path = 'products/hail_20210620_r1_test_4/pkl/'\n",
    "\n",
    "if not os.path.exists(mask_array_path):\n",
    "    os.makedirs(mask_array_path)  \n",
    "\n",
    "all_images = glob.glob(images_path+'*.png')\n",
    "all_images.sort()\n",
    "\n",
    "for file in all_images:\n",
    "    im = cv2.imread(file)\n",
    "    outputs = predictor(im)\n",
    "    masks = outputs['instances'][outputs['instances'].pred_classes==0].pred_masks.cpu().numpy()\n",
    "\n",
    "    image_name = Path(file).stem\n",
    "\n",
    "    mask_array = []\n",
    "    #hail_count = 1 # For saving all individual masks to a figure\n",
    "    for i in range(masks.shape[0]):\n",
    "        mask_int = masks[i,:,:]*1\n",
    "        mask_array.append(mask_int)\n",
    "        \n",
    "        #hail_area = (mask_int == 1).sum() #1 pixel = 1mm^2\n",
    "        #hail_areas.append(hail_area)\n",
    "        #title='Hailstone area: '+str(hail_area)+'mm$^2$'\n",
    "        #display_images([masks[:,:,i]],mask_path, image_name, hail_count, titles=[title])\n",
    "        #plt.close('all')\n",
    "        #hail_count = hail_count + 1\n",
    "\n",
    "    with open(mask_array_path+'mask_array_'+image_name+'.pkl','wb') as f:\n",
    "        pickle.dump(mask_array, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
