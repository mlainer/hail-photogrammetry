{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0595ca50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle as pk\n",
    "import os\n",
    "\n",
    "hd_part1 = '/scratch/mlainer/data/hail/detectron2/products/hparam_tuning/run-3/hail_20210620_r1/all_images/hail_sizes/part1/hail_dict.pkl'\n",
    "hd_part2 = '/scratch/mlainer/data/hail/detectron2/products/hparam_tuning/run-3/hail_20210620_r1/all_images/hail_sizes/part2/hail_dict.pkl'\n",
    "hd_part3 = '/scratch/mlainer/data/hail/detectron2/products/hparam_tuning/run-3/hail_20210620_r1/all_images/hail_sizes/part3/hail_dict.pkl'\n",
    "\n",
    "#List of indexes which are no hail:\n",
    "no_hail_part1 = [3751] # leaves etc.\n",
    "no_hail_part2 = [2939, 3250]\n",
    "\n",
    "with open(hd_part1,'rb') as f:\n",
    "    hd_p1 = pk.load(f)\n",
    "\n",
    "for ind in no_hail_part1:\n",
    "    hd_p1['major_ma'][ind] = np.nan\n",
    "    hd_p1['minor_ma'][ind] = np.nan\n",
    "    hd_p1['L_center'][ind] = np.nan    \n",
    "    \n",
    "with open(hd_part2,'rb') as f:\n",
    "    hd_p2 = pk.load(f)\n",
    "\n",
    "for ind in no_hail_part2:\n",
    "    hd_p2['major_ma'][ind] = np.nan\n",
    "    hd_p2['minor_ma'][ind] = np.nan\n",
    "    hd_p2['L_center'][ind] = np.nan    \n",
    "    \n",
    "with open(hd_part3,'rb') as f:\n",
    "    hd_p3 = pk.load(f)\n",
    "    \n",
    "ds = [hd_p1, hd_p2, hd_p3]\n",
    "d = {}\n",
    "for k in ['L_center', 'major_ma', 'minor_ma','hail_co','tile_names','img']:\n",
    "    d[k] = tuple(d[k] for d in ds)\n",
    "\n",
    "L_center = np.hstack(np.asarray(d['L_center'][:], dtype=object))\n",
    "names = np.hstack(np.asarray(d['tile_names'][:], dtype=object))\n",
    "\n",
    "major_ax = np.hstack(np.asarray(d['major_ma'][:], dtype=object))\n",
    "minor_ax =  np.hstack(np.asarray(d['minor_ma'][:], dtype=object))\n",
    "hail_co_x = np.hstack([d['hail_co'][0][0],d['hail_co'][1][0],d['hail_co'][2][0]])\n",
    "hail_co_y = np.hstack([d['hail_co'][0][1],d['hail_co'][1][1],d['hail_co'][2][1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9001059-a2e5-457c-9f8f-4e5dacd42171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected hail stones:  18209\n"
     ]
    }
   ],
   "source": [
    "#Print number of detected hail stones\n",
    "print(\"Detected hail stones: \", np.sum(~np.isnan(major_ax)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f4e54cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export results to CSV file\n",
    "import pandas as pd\n",
    "\n",
    "data={'major_ax': major_ax, 'minor_ax': minor_ax, 'x_cord': hail_co_x, 'y_cord': hail_co_y}\n",
    "df=pd.DataFrame(data)\n",
    "df.index.name = 'hail_index'\n",
    "df.to_csv('/home/martin/container/hail/jerome/hail_20210620_r1.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3b8b340b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.04888e+07\n",
      "Area:  752.6733\n"
     ]
    }
   ],
   "source": [
    "#Get number of transparent pixels in orthophoto \n",
    "stream = os.popen('convert /home/martin/container/hail/jerome/odm_orthophoto.png -alpha extract -negate -format \"%[fx:mean*w*h]\" info:')\n",
    "output = stream.read()\n",
    "print(output)\n",
    "\n",
    "all_px = 5.72271*1e8\n",
    "rel_px = all_px - np.float(output)\n",
    "area = rel_px*1.5/1e6\n",
    "print('Area: ', area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e7b02732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "black:  115232530\n",
      "Area:  602.651205\n"
     ]
    }
   ],
   "source": [
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "#Load orthophoto image   \n",
    "orthophoto_file = '/home/martin/container/autofs/data/hail/hail_20210620_r5/odm_orthophoto/odm_20210620_r5.png'\n",
    "PIL.Image.MAX_IMAGE_PIXELS = 8046500000\n",
    "image = np.asarray(Image.open(orthophoto_file))\n",
    "\n",
    "sought = [0, 0, 0]\n",
    "black = np.count_nonzero(np.all(image[:,:,0:3] == sought, axis = 2))\n",
    "print(\"Black area: \", black)\n",
    "\n",
    "rel_px = np.size(image,0)*np.size(image,1) - black\n",
    "area = rel_px*1.5/1e6\n",
    "print('Relevant area: ', area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "435bf1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write results to netcdf file\n",
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "\n",
    "path_nc = '/home/martin/container/hail/jerome/'\n",
    "if not os.path.exists(path_nc):\n",
    "    os.makedirs(path_nc)\n",
    "        \n",
    "fn = path_nc+'hail_20210620_r1.nc'\n",
    "ds = nc.Dataset(fn, 'w', format='NETCDF4')\n",
    "\n",
    "major_ax_nc = ds.createVariable('major_ax', 'f4')\n",
    "major_ax_nc.units = 'mm'\n",
    "\n",
    "minor_ax_nc = ds.createVariable('minor_ax', 'f4')\n",
    "minor_ax_nc.units = 'mm'\n",
    "\n",
    "hail_x = ds.createVariable('hail_x', 'int')\n",
    "hail_x.units = 'pixel'\n",
    "hail_y = ds.createVariable('hail_y', 'int')\n",
    "hail_y.units = 'pixel'\n",
    "\n",
    "major_ax_nc = major_ax\n",
    "minor_ax_nc = minor_ax\n",
    "hail_x = hail_co_x\n",
    "hail_y = hail_co_y\n",
    "\n",
    "ds.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
